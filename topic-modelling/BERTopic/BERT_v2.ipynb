{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60dfa774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
    "from bertopic.vectorizers import ClassTfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf89f2",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb38c3",
   "metadata": {},
   "source": [
    "**Preprocessing:**\n",
    "\n",
    "Emoji removal – strips out emoticons, symbols, pictographs, and flags.\n",
    "\n",
    "Missing/invalid values handling – replaces NaN, None, 'nan', 'null', etc. with empty text.\n",
    "\n",
    "Normalization – converts text to lowercase and trims whitespace.\n",
    "\n",
    "Noise removal – deletes URLs, reduces repeated punctuation, and collapses extra spaces.\n",
    "\n",
    "Token filtering – removes very short or likely-typo words while keeping important short words (e.g., \"ok\", \"hi\").\n",
    "\n",
    "Dataset-level cleaning – ensures the Comments column exists, preprocesses each entry, and removes empty or too-short results.\n",
    "\n",
    "Final output – provides a validated list of sufficiently long, clean comments ready for topic modeling or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4406f80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Original number of comments: 978\n",
      "Number of valid comments after cleaning: 969\n",
      "\n",
      "Sample cleaned comments:\n",
      "  1: bat walang gumagalaw kay romualdez eh siya nga pinaka suspicious dyan...\n",
      "  2: kunyari hindi alam....\n",
      "  3: question before mag release nang full payment wla man lang inspection?...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "# Enhanced preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Enhanced preprocessing for social media comments\"\"\"\n",
    "    # Handle various data types and missing values\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and handle numpy types\n",
    "    text = str(text).strip()\n",
    "    \n",
    "    if text == \"\" or text.lower() in ['nan', 'none', 'null']:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove emojis\n",
    "    text = remove_emojis(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove excessive punctuation but keep some for context\n",
    "    text = re.sub(r'[.]{2,}', '.', text)\n",
    "    text = re.sub(r'[!]{2,}', '!', text)\n",
    "    text = re.sub(r'[?]{2,}', '?', text)\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove very short words (likely typos) but keep important short words\n",
    "    important_short = {'ok', 'no', 'go', 'hi', 'me', 'we', 'he', 'so', 'up', 'my', 'is', 'at', 'it', 'on', 'or'}\n",
    "    words = text.split()\n",
    "    words = [word for word in words if len(word) >= 2 or word in important_short]\n",
    "    \n",
    "    result = ' '.join(words).strip()\n",
    "    return result if result else \"\"\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "document = pd.read_csv('../../DATA MINING - DATASET - Consolidated_Dataset (1).csv')\n",
    "\n",
    "# Ensure Comments column exists and handle missing values\n",
    "if 'Comments' not in document.columns:\n",
    "    print(\"Available columns:\", document.columns.tolist())\n",
    "    raise ValueError(\"'Comments' column not found in the dataset\")\n",
    "\n",
    "# Handle missing values and data type issues\n",
    "document['Comments'] = document['Comments'].fillna('')  # Fill NaN with empty string\n",
    "document['Comments'] = document['Comments'].astype(str)  # Ensure all are strings\n",
    "\n",
    "print(f\"Original number of comments: {len(document)}\")\n",
    "\n",
    "# Clean the comments\n",
    "document['Comments_Clean'] = document['Comments'].apply(preprocess_text)\n",
    "\n",
    "# Remove empty comments after preprocessing and ensure we have strings\n",
    "document = document[\n",
    "    (document['Comments_Clean'].str.len() > 5) & \n",
    "    (document['Comments_Clean'] != '') & \n",
    "    (document['Comments_Clean'].notna())\n",
    "]\n",
    "\n",
    "comments = document['Comments_Clean'].tolist()\n",
    "\n",
    "# Final validation - ensure all comments are non-empty strings\n",
    "comments = [str(comment).strip() for comment in comments if comment and str(comment).strip()]\n",
    "comments = [comment for comment in comments if len(comment) > 5]\n",
    "\n",
    "print(f\"Number of valid comments after cleaning: {len(comments)}\")\n",
    "\n",
    "if len(comments) < 10:\n",
    "    raise ValueError(f\"Too few valid comments ({len(comments)}). Need at least 10 for topic modeling.\")\n",
    "\n",
    "# Debug: Show sample of cleaned comments\n",
    "print(\"\\nSample cleaned comments:\")\n",
    "for i, comment in enumerate(comments[:3]):\n",
    "    print(f\"  {i+1}: {comment[:100]}...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3502fae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total merged stopwords: 511\n"
     ]
    }
   ],
   "source": [
    "# Enhanced stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "with open(\"../../tagalog_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tagalog_stopwords = set([line.strip().lower() for line in f if line.strip()])\n",
    "\n",
    "# Add social media specific stopwords\n",
    "social_media_stopwords = {\n",
    "    'lol', 'lmao', 'haha', 'hehe', 'omg', 'wtf', 'tbh', 'imo', 'imho',\n",
    "    'rt', 'dm', 'pm', 'fb', 'ig', 'twitter', 'facebook', 'instagram',\n",
    "    'like', 'share', 'comment', 'follow', 'retweet', 'post', 'tagged'\n",
    "}\n",
    "\n",
    "all_stopwords = list(eng_stopwords | tagalog_stopwords | social_media_stopwords)\n",
    "print(f\"Total merged stopwords: {len(all_stopwords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "56e6deeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1440\n",
      "Document-term matrix shape: (969, 1440)\n",
      "Using min_cluster_size: 2\n"
     ]
    }
   ],
   "source": [
    "# Improved vectorizer with TF-IDF\n",
    "n_docs = len(comments)\n",
    "\n",
    "# Calculate min_df and max_df properly to avoid conflicts\n",
    "min_df_value = 2\n",
    "max_df_value = 0.8\n",
    "\n",
    "vectorizer_model = TfidfVectorizer(\n",
    "    stop_words=all_stopwords,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=min_df_value,\n",
    "    max_df=max_df_value,\n",
    "    max_features=5000,\n",
    "    token_pattern=r'(?u)\\b[a-zA-Z][a-zA-Z]+\\b',  # Only alphabetic tokens with 2+ chars\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "# Optimized UMAP parameters for small dataset\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=min(10, max(2, int(n_docs * 0.015))),  # Adaptive to dataset size\n",
    "    n_components=5,  # Reduced dimensions for small dataset\n",
    "    min_dist=0.1,  # Slightly higher to prevent over-clustering\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Optimized HDBSCAN parameters\n",
    "vec = vectorizer_model.fit(comments)\n",
    "X = vectorizer_model.transform(comments)\n",
    "min_cluster_size = 2\n",
    "print(\"Vocab size:\", len(vec.vocabulary_))\n",
    "print(\"Document-term matrix shape:\", X.shape)\n",
    "print(f\"Using min_cluster_size: {min_cluster_size}\")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples=1,  # More flexible clustering\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True,\n",
    "    cluster_selection_epsilon=0.1  # Allow more flexible cluster selection\n",
    ")\n",
    "\n",
    "# Better sentence transformer for multilingual content\n",
    "sentence_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Enhanced representation models\n",
    "keybert_model = KeyBERTInspired()\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# Custom c-TF-IDF model\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "66300058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 22:42:34,745 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERTopic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 31/31 [00:16<00:00,  1.83it/s]\n",
      "2025-09-19 22:42:51,707 - BERTopic - Embedding - Completed ✓\n",
      "2025-09-19 22:42:51,707 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-19 22:42:52,838 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-19 22:42:52,840 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-19 22:42:52,924 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-19 22:42:52,926 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-09-19 22:42:52,975 - BERTopic - Representation - Completed ✓\n",
      "2025-09-19 22:42:52,975 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-09-19 22:42:52,980 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-19 22:42:53,023 - BERTopic - Representation - Completed ✓\n",
      "2025-09-19 22:42:53,025 - BERTopic - Topic reduction - Reduced number of topics from 12 to 12\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERTopic with enhanced configuration\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=sentence_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    language=None,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    nr_topics=\"auto\"\n",
    ")\n",
    "print(\"Training BERTopic model...\")\n",
    "topics, probs = topic_model.fit_transform(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3059882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full topic info DataFrame\n",
    "topic_model.get_topic_info().to_csv(\"topic_info_BERT_v2.csv\", index=False)\n",
    "\n",
    "# Save top words for each topic\n",
    "import pandas as pd\n",
    "rows = []\n",
    "for topic_num in topic_model.get_topic_info()['Topic']:\n",
    "    if topic_num == -1:  # skip outliers\n",
    "        continue\n",
    "    words = topic_model.get_topic(topic_num)\n",
    "    rows.append({\n",
    "        \"Topic\": topic_num,\n",
    "        \"Keywords\": \", \".join([w for w, _ in words])\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows).to_csv(\"topic_keywords_BERT_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "292a2e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of topics found: 11\n",
      "Number of outliers (topic -1): 164\n",
      "\n",
      "Topic Information:\n",
      "Topic -1: 164 documents\n",
      "\n",
      "Topic 0: 324 documents\n",
      "  Top words: galing, pilipinas, makukulong, alam, senado, pilipino, siguro, kulungan, bayan, kapal\n",
      "\n",
      "Topic 1: 177 documents\n",
      "  Top words: guy, alcantara, good, coming, bangag, hopefully, tiba, thank, romualdez, properties\n",
      "\n",
      "Topic 2: 71 documents\n",
      "  Top words: corruption, take, government, people, money, fund, president, corrupt, confidential, philippine\n",
      "\n",
      "Topic 3: 64 documents\n",
      "  Top words: congressman, magalong, congressmen, senators, congress, impeachment, mayors, mayor, senate, term\n",
      "\n",
      "Topic 4: 57 documents\n",
      "  Top words: flood control, flood, control, control projects, inspect, sapat, projects, road, climate, admin\n",
      "\n",
      "Topic 5: 29 documents\n",
      "  Top words: inspection, construction, contractor, completion, build, luxury cars, bypass, complete, engr, project\n",
      "\n",
      "Topic 6: 25 documents\n",
      "  Top words: state, witness, hearing, gustong, witness discaya, marcobeta, gawing, tapos, itong, mataas\n",
      "\n",
      "Topic 7: 21 documents\n",
      "  Top words: death penalty, penalty, death, jail, put, children, prison, execution, bitayin, public\n",
      "\n",
      "Topic 8: 13 documents\n",
      "  Top words: freeze, assets, freeze assets, escape, seize, seize assets, came, list, tax, many days\n",
      "\n",
      "Overall Coherence Scores:\n",
      "C_v: 0.4257\n",
      "UMass: -14.5700\n",
      "\n",
      "Model Statistics:\n",
      "Total documents: 969\n",
      "Average documents per topic: 88.1\n"
     ]
    }
   ],
   "source": [
    "# Get topic information\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(f\"\\nNumber of topics found: {len(topic_info) - 1}\")  # -1 to exclude outlier topic\n",
    "print(f\"Number of outliers (topic -1): {sum(1 for t in topics if t == -1)}\")\n",
    "\n",
    "# Display topic information\n",
    "print(\"\\nTopic Information:\")\n",
    "for i, row in topic_info.head(10).iterrows():\n",
    "    print(f\"Topic {row['Topic']}: {row['Count']} documents\")\n",
    "    if row['Topic'] != -1:  # Skip outlier topic\n",
    "        words = topic_model.get_topic(row['Topic'])\n",
    "        print(f\"  Top words: {', '.join([word for word, _ in words[:10]])}\")\n",
    "    print()\n",
    "\n",
    "# Calculate and display coherence scores (if available)\n",
    "try:\n",
    "    from gensim.corpora import Dictionary\n",
    "    from gensim.models import CoherenceModel\n",
    "    from gensim.utils import simple_preprocess\n",
    "    \n",
    "    # Prepare documents for coherence calculation\n",
    "    processed_docs = [simple_preprocess(doc) for doc in comments]\n",
    "    dictionary = Dictionary(processed_docs)\n",
    "    \n",
    "    # Get topics for coherence calculation\n",
    "    topics_for_coherence = []\n",
    "    for topic_id in range(len(topic_info) - 1):  # Exclude outlier topic\n",
    "        if topic_id != -1:\n",
    "            topic_words = [word for word, _ in topic_model.get_topic(topic_id)]\n",
    "            topics_for_coherence.append(topic_words[:10])  # Top 10 words per topic\n",
    "    \n",
    "    if topics_for_coherence:\n",
    "        # Calculate coherence\n",
    "        coherence_model_cv = CoherenceModel(\n",
    "            topics=topics_for_coherence, \n",
    "            texts=processed_docs, \n",
    "            dictionary=dictionary, \n",
    "            coherence='c_v'\n",
    "        )\n",
    "        coherence_cv = coherence_model_cv.get_coherence()\n",
    "        \n",
    "        coherence_model_umass = CoherenceModel(\n",
    "            topics=topics_for_coherence, \n",
    "            texts=processed_docs, \n",
    "            dictionary=dictionary, \n",
    "            coherence='u_mass'\n",
    "        )\n",
    "        coherence_umass = coherence_model_umass.get_coherence()\n",
    "        \n",
    "        print(f\"Overall Coherence Scores:\")\n",
    "        print(f\"C_v: {coherence_cv:.4f}\")\n",
    "        print(f\"UMass: {coherence_umass:.4f}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Gensim not available for coherence calculation. Install with: pip install gensim\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate coherence scores: {e}\")\n",
    "\n",
    "# Additional model analysis\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"Total documents: {len(comments)}\")\n",
    "print(f\"Average documents per topic: {len(comments) / max(1, len(set(topics)) - (1 if -1 in topics else 0)):.1f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
