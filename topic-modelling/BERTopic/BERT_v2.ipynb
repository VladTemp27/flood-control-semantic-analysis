{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dfa774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Isaac\\Documents\\Visual Studio Code\\flood-control-semantic-analysis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
    "from bertopic.vectorizers import ClassTfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf89f2",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb38c3",
   "metadata": {},
   "source": [
    "**Preprocessing:**\n",
    "\n",
    "Emoji removal – strips out emoticons, symbols, pictographs, and flags.\n",
    "\n",
    "Missing/invalid values handling – replaces NaN, None, 'nan', 'null', etc. with empty text.\n",
    "\n",
    "Normalization – converts text to lowercase and trims whitespace.\n",
    "\n",
    "Noise removal – deletes URLs, reduces repeated punctuation, and collapses extra spaces.\n",
    "\n",
    "Token filtering – removes very short or likely-typo words while keeping important short words (e.g., \"ok\", \"hi\").\n",
    "\n",
    "Dataset-level cleaning – ensures the Comments column exists, preprocesses each entry, and removes empty or too-short results.\n",
    "\n",
    "Final output – provides a validated list of sufficiently long, clean comments ready for topic modeling or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4406f80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Original number of comments: 978\n",
      "Number of valid comments after cleaning: 969\n",
      "\n",
      "Sample cleaned comments:\n",
      "  1: bat walang gumagalaw kay romualdez eh siya nga pinaka suspicious dyan...\n",
      "  2: kunyari hindi alam....\n",
      "  3: question before mag release nang full payment wla man lang inspection?...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "# Enhanced preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Enhanced preprocessing for social media comments\"\"\"\n",
    "    # Handle various data types and missing values\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and handle numpy types\n",
    "    text = str(text).strip()\n",
    "    \n",
    "    if text == \"\" or text.lower() in ['nan', 'none', 'null']:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove emojis\n",
    "    text = remove_emojis(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove excessive punctuation but keep some for context\n",
    "    text = re.sub(r'[.]{2,}', '.', text)\n",
    "    text = re.sub(r'[!]{2,}', '!', text)\n",
    "    text = re.sub(r'[?]{2,}', '?', text)\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove very short words (likely typos) but keep important short words\n",
    "    important_short = {'ok', 'no', 'go', 'hi', 'me', 'we', 'he', 'so', 'up', 'my', 'is', 'at', 'it', 'on', 'or'}\n",
    "    words = text.split()\n",
    "    words = [word for word in words if len(word) >= 2 or word in important_short]\n",
    "    \n",
    "    result = ' '.join(words).strip()\n",
    "    return result if result else \"\"\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "document = pd.read_csv('../../DATA MINING - DATASET - Consolidated_Dataset (1).csv')\n",
    "\n",
    "# Ensure Comments column exists and handle missing values\n",
    "if 'Comments' not in document.columns:\n",
    "    print(\"Available columns:\", document.columns.tolist())\n",
    "    raise ValueError(\"'Comments' column not found in the dataset\")\n",
    "\n",
    "# Handle missing values and data type issues\n",
    "document['Comments'] = document['Comments'].fillna('')  # Fill NaN with empty string\n",
    "document['Comments'] = document['Comments'].astype(str)  # Ensure all are strings\n",
    "\n",
    "print(f\"Original number of comments: {len(document)}\")\n",
    "\n",
    "# Clean the comments\n",
    "document['Comments_Clean'] = document['Comments'].apply(preprocess_text)\n",
    "\n",
    "# Remove empty comments after preprocessing and ensure we have strings\n",
    "document = document[\n",
    "    (document['Comments_Clean'].str.len() > 5) & \n",
    "    (document['Comments_Clean'] != '') & \n",
    "    (document['Comments_Clean'].notna())\n",
    "]\n",
    "\n",
    "comments = document['Comments_Clean'].tolist()\n",
    "\n",
    "# Final validation - ensure all comments are non-empty strings\n",
    "comments = [str(comment).strip() for comment in comments if comment and str(comment).strip()]\n",
    "comments = [comment for comment in comments if len(comment) > 5]\n",
    "\n",
    "print(f\"Number of valid comments after cleaning: {len(comments)}\")\n",
    "\n",
    "if len(comments) < 10:\n",
    "    raise ValueError(f\"Too few valid comments ({len(comments)}). Need at least 10 for topic modeling.\")\n",
    "\n",
    "# Debug: Show sample of cleaned comments\n",
    "print(\"\\nSample cleaned comments:\")\n",
    "for i, comment in enumerate(comments[:3]):\n",
    "    print(f\"  {i+1}: {comment[:100]}...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb116c",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837208f",
   "metadata": {},
   "source": [
    "NLTK's English Stopwords + Social Media Stopwords + Custom Tagalog Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3502fae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total merged stopwords: 511\n"
     ]
    }
   ],
   "source": [
    "# Enhanced stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "with open(\"tagalog_stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tagalog_stopwords = set([line.strip().lower() for line in f if line.strip()])\n",
    "\n",
    "# Add social media specific stopwords\n",
    "social_media_stopwords = {\n",
    "    'lol', 'lmao', 'haha', 'hehe', 'omg', 'wtf', 'tbh', 'imo', 'imho',\n",
    "    'rt', 'dm', 'pm', 'fb', 'ig', 'twitter', 'facebook', 'instagram',\n",
    "    'like', 'share', 'comment', 'follow', 'retweet', 'post', 'tagged'\n",
    "}\n",
    "\n",
    "all_stopwords = list(eng_stopwords | tagalog_stopwords | social_media_stopwords)\n",
    "print(f\"Total merged stopwords: {len(all_stopwords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3cad73",
   "metadata": {},
   "source": [
    "# BERTopic Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b599fa0a",
   "metadata": {},
   "source": [
    "TF-IDF Vectorizer – builds vocab (1–2 n-grams), removes stopwords, filters rare/common terms.\n",
    "\n",
    "UMAP – reduces dimensionality with adaptive neighbors for small datasets.\n",
    "\n",
    "HDBSCAN – flexible clustering with small min_cluster_size.\n",
    "\n",
    "Sentence Transformer – multilingual embeddings.\n",
    "\n",
    "Representation Models – KeyBERT, MMR, and custom c-TF-IDF for keyword extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e6deeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1440\n",
      "Document-term matrix shape: (969, 1440)\n",
      "Using min_cluster_size: 2\n"
     ]
    }
   ],
   "source": [
    "# Improved vectorizer with TF-IDF\n",
    "n_docs = len(comments)\n",
    "\n",
    "# Calculate min_df and max_df properly to avoid conflicts\n",
    "min_df_value = 2\n",
    "max_df_value = 0.8\n",
    "\n",
    "vectorizer_model = TfidfVectorizer(\n",
    "    stop_words=all_stopwords,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=min_df_value,\n",
    "    max_df=max_df_value,\n",
    "    max_features=5000,\n",
    "    token_pattern=r'(?u)\\b[a-zA-Z][a-zA-Z]+\\b',  # Only alphabetic tokens with 2+ chars\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "# Optimized UMAP parameters for small dataset\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=min(10, max(2, int(n_docs * 0.015))),  # Adaptive to dataset size\n",
    "    n_components=5,  # Reduced dimensions for small dataset\n",
    "    min_dist=0.1,  # Slightly higher to prevent over-clustering\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Optimized HDBSCAN parameters\n",
    "vec = vectorizer_model.fit(comments)\n",
    "X = vectorizer_model.transform(comments)\n",
    "min_cluster_size = 2\n",
    "print(\"Vocab size:\", len(vec.vocabulary_))\n",
    "print(\"Document-term matrix shape:\", X.shape)\n",
    "print(f\"Using min_cluster_size: {min_cluster_size}\")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples=1,  # More flexible clustering\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True,\n",
    "    cluster_selection_epsilon=0.1  # Allow more flexible cluster selection\n",
    ")\n",
    "\n",
    "# Better sentence transformer for multilingual content\n",
    "sentence_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Enhanced representation models\n",
    "keybert_model = KeyBERTInspired()\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# Custom c-TF-IDF model\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f96632",
   "metadata": {},
   "source": [
    "## Create the model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66300058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 21:49:25,527 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERTopic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 31/31 [00:34<00:00,  1.12s/it]\n",
      "2025-09-20 21:50:00,159 - BERTopic - Embedding - Completed ✓\n",
      "2025-09-20 21:50:00,160 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-20 21:50:15,933 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-20 21:50:15,936 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-20 21:50:16,041 - BERTopic - Cluster - Completed ✓\n",
      "2025-09-20 21:50:16,043 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-09-20 21:50:16,145 - BERTopic - Representation - Completed ✓\n",
      "2025-09-20 21:50:16,146 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-09-20 21:50:16,152 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-20 21:50:16,200 - BERTopic - Representation - Completed ✓\n",
      "2025-09-20 21:50:16,202 - BERTopic - Topic reduction - Reduced number of topics from 11 to 9\n"
     ]
    }
   ],
   "source": [
    "topic_model = BERTopic(\n",
    "    embedding_model=sentence_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    language=None,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    nr_topics=\"auto\"\n",
    ")\n",
    "print(\"Training BERTopic model...\")\n",
    "topics, probs = topic_model.fit_transform(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddd965",
   "metadata": {},
   "source": [
    "### Save topic info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3059882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full topic info DataFrame\n",
    "topic_model.get_topic_info().to_csv(\"topic_info_BERT_v2.csv\", index=False)\n",
    "\n",
    "# Save top words for each topic\n",
    "import pandas as pd\n",
    "rows = []\n",
    "for topic_num in topic_model.get_topic_info()['Topic']:\n",
    "    if topic_num == -1:  # skip outliers\n",
    "        continue\n",
    "    words = topic_model.get_topic(topic_num)\n",
    "    rows.append({\n",
    "        \"Topic\": topic_num,\n",
    "        \"Keywords\": \", \".join([w for w, _ in words])\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows).to_csv(\"topic_keywords_BERT_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "292a2e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of topics found: 8\n",
      "Number of outliers (topic -1): 124\n",
      "\n",
      "Topic Information:\n",
      "Topic -1: 124 documents\n",
      "\n",
      "Topic 0: 394 documents\n",
      "  Top words: alam, pilipinas, tapos, galing, pilipino, makukulong, kawawa, senado, bayan, inspection\n",
      "\n",
      "Topic 1: 183 documents\n",
      "  Top words: talk, alcantara, good, bangag, coming, thank, romualdez, sabay, tiba, properties\n",
      "\n",
      "Topic 2: 103 documents\n",
      "  Top words: state, witness, coa, congressman, senador, senate, presidente, mayor, magalong, hearing\n",
      "\n",
      "Topic 3: 69 documents\n",
      "  Top words: corruption, take, government, people, corrupt, even, money, billions, filipinos, philippine\n",
      "\n",
      "Topic 4: 53 documents\n",
      "  Top words: flood control, flood, control, control project, control projects, projects, project, budget, sapat, admin\n",
      "\n",
      "Topic 5: 19 documents\n",
      "  Top words: death penalty, penalty, death, jail, prison, come, put, execution, bitayin, money\n",
      "\n",
      "Topic 6: 13 documents\n",
      "  Top words: freeze, assets, freeze assets, seize, seize assets, escape, list, came, tax, lumabas\n",
      "\n",
      "Topic 7: 11 documents\n",
      "  Top words: hague, duterte, puwede, sina, digong, agree, mean, jailed, two, true\n",
      "\n",
      "Overall Coherence Scores:\n",
      "C_v: 0.4695\n",
      "UMass: -13.5247\n",
      "\n",
      "Model Statistics:\n",
      "Total documents: 969\n",
      "Average documents per topic: 121.1\n"
     ]
    }
   ],
   "source": [
    "# Get topic information\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(f\"\\nNumber of topics found: {len(topic_info) - 1}\")  # -1 to exclude outlier topic\n",
    "print(f\"Number of outliers (topic -1): {sum(1 for t in topics if t == -1)}\")\n",
    "\n",
    "# Display topic information\n",
    "print(\"\\nTopic Information:\")\n",
    "for i, row in topic_info.head(10).iterrows():\n",
    "    print(f\"Topic {row['Topic']}: {row['Count']} documents\")\n",
    "    if row['Topic'] != -1:  # Skip outlier topic\n",
    "        words = topic_model.get_topic(row['Topic'])\n",
    "        print(f\"  Top words: {', '.join([word for word, _ in words[:10]])}\")\n",
    "    print()\n",
    "\n",
    "# Calculate and display coherence scores (if available)\n",
    "try:\n",
    "    from gensim.corpora import Dictionary\n",
    "    from gensim.models import CoherenceModel\n",
    "    from gensim.utils import simple_preprocess\n",
    "    \n",
    "    # Prepare documents for coherence calculation\n",
    "    processed_docs = [simple_preprocess(doc) for doc in comments]\n",
    "    dictionary = Dictionary(processed_docs)\n",
    "    \n",
    "    # Get topics for coherence calculation\n",
    "    topics_for_coherence = []\n",
    "    for topic_id in range(len(topic_info) - 1):  # Exclude outlier topic\n",
    "        if topic_id != -1:\n",
    "            topic_words = [word for word, _ in topic_model.get_topic(topic_id)]\n",
    "            topics_for_coherence.append(topic_words[:10])  # Top 10 words per topic\n",
    "    \n",
    "    if topics_for_coherence:\n",
    "        # Calculate coherence\n",
    "        coherence_model_cv = CoherenceModel(\n",
    "            topics=topics_for_coherence, \n",
    "            texts=processed_docs, \n",
    "            dictionary=dictionary, \n",
    "            coherence='c_v'\n",
    "        )\n",
    "        coherence_cv = coherence_model_cv.get_coherence()\n",
    "        \n",
    "        coherence_model_umass = CoherenceModel(\n",
    "            topics=topics_for_coherence, \n",
    "            texts=processed_docs, \n",
    "            dictionary=dictionary, \n",
    "            coherence='u_mass'\n",
    "        )\n",
    "        coherence_umass = coherence_model_umass.get_coherence()\n",
    "        \n",
    "        print(f\"Overall Coherence Scores:\")\n",
    "        print(f\"C_v: {coherence_cv:.4f}\")\n",
    "        print(f\"UMass: {coherence_umass:.4f}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Gensim not available for coherence calculation. Install with: pip install gensim\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate coherence scores: {e}\")\n",
    "\n",
    "# Additional model analysis\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"Total documents: {len(comments)}\")\n",
    "print(f\"Average documents per topic: {len(comments) / max(1, len(set(topics)) - (1 if -1 in topics else 0)):.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dda4588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 21:54:08,756 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "topic_model.save(\"models/BERTv2_model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27358e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full topic info DataFrame\n",
    "topic_model.get_topic_info().to_csv(\"topic_info_BERTv2_merged.csv\", index=False)\n",
    "\n",
    "# Save top words for each topic\n",
    "import pandas as pd\n",
    "rows = []\n",
    "for topic_num in topic_model.get_topic_info()['Topic']:\n",
    "    if topic_num == -1:  # skip outliers\n",
    "        continue\n",
    "    words = topic_model.get_topic(topic_num)\n",
    "    rows.append({\n",
    "        \"Topic\": topic_num,\n",
    "        \"Keywords\": \", \".join([w for w, _ in words])\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows).to_csv(\"topic_keywords_BERTv2_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122187a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_diversity(topic_model):\n",
    "    \"\"\"\n",
    "    Calculate topic diversity: proportion of unique words across all topic keywords.\n",
    "    \"\"\"\n",
    "    topics = topic_model.get_topics()\n",
    "    all_words = []\n",
    "    for topic in topics.values():\n",
    "        words = [word for word, _ in topic]\n",
    "        all_words.extend(words)\n",
    "    unique_words = set(all_words)\n",
    "    return len(unique_words) / len(all_words) if all_words else 0.0\n",
    "\n",
    "\n",
    "def topic_silhouette(embeddings, topics):\n",
    "    \"\"\"\n",
    "    Compute silhouette score for topic assignments, excluding outliers (-1).\n",
    "    Handles list-based topic labels.\n",
    "    \"\"\"\n",
    "    topics = np.array(topics)  # convert list to array\n",
    "    \n",
    "    # Exclude outliers\n",
    "    mask = topics != -1\n",
    "    if np.sum(mask) < 2 or len(np.unique(topics[mask])) < 2:\n",
    "        return None\n",
    "    \n",
    "    return silhouette_score(embeddings[mask], topics[mask])\n",
    "\n",
    "\n",
    "def intra_topic_distance(topic_model, embeddings, documents):\n",
    "    \"\"\"\n",
    "    Average pairwise distance within each topic.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_distances\n",
    "    topics, _ = topic_model.transform(documents)\n",
    "    unique_topics = set(topics) - {-1}\n",
    "\n",
    "    intra_distances = []\n",
    "    for t in unique_topics:\n",
    "        idx = np.where(topics == t)[0]\n",
    "        if len(idx) > 1:\n",
    "            dists = cosine_distances(embeddings[idx])\n",
    "            intra_distances.append(np.mean(dists))\n",
    "    return np.mean(intra_distances) if intra_distances else None\n",
    "\n",
    "\n",
    "def inter_topic_distance(topic_model):\n",
    "    \"\"\"\n",
    "    Distance between topic centroids (based on c-TF-IDF representations).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "    topic_embeddings = topic_model.c_tf_idf_\n",
    "    if topic_embeddings is None:\n",
    "        return None\n",
    "    dists = cosine_distances(topic_embeddings)\n",
    "    return np.mean(dists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "302b73f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 31/31 [00:33<00:00,  1.07s/it]\n",
      "Batches: 100%|██████████| 31/31 [00:32<00:00,  1.04s/it]\n",
      "2025-09-20 21:58:07,317 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-09-20 21:58:07,326 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-09-20 21:58:07,329 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-09-20 21:58:07,369 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
      "2025-09-20 21:58:07,483 - BERTopic - Probabilities - Completed ✓\n",
      "2025-09-20 21:58:07,484 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Diversity: 0.9888888888888889\n",
      "Silhouette Score: 0.0683543011546135\n",
      "Intra-topic Distance: 0.4795085\n",
      "Inter-topic Distance: 0.7339283702415055\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings\n",
    "embeddings = sentence_model.encode(comments, show_progress_bar=True)\n",
    "\n",
    "# Run metrics\n",
    "div_score = topic_diversity(topic_model)\n",
    "sil_score = topic_silhouette(embeddings, topics)\n",
    "intra_dist = intra_topic_distance(topic_model, embeddings, comments)\n",
    "inter_dist = inter_topic_distance(topic_model)\n",
    "\n",
    "print(\"Topic Diversity:\", div_score)\n",
    "print(\"Silhouette Score:\", sil_score)\n",
    "print(\"Intra-topic Distance:\", intra_dist)\n",
    "print(\"Inter-topic Distance:\", inter_dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
